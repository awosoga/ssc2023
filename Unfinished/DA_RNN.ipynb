{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awosoga/ssc2023/blob/main/Unfinished/DA_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import product\n",
        "from tensorflow import summary"
      ],
      "metadata": {
        "id": "Li6MDV7G7J_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gLu1LId6yKF"
      },
      "outputs": [],
      "source": [
        "input_path = \"full_data.csv\"  # Replace with your CSV file path\n",
        "\n",
        "\n",
        "def read_data(input_path, debug=False, diff=False):\n",
        "    \"\"\"\n",
        "    This function reads the timeseries data from a file and then normalizes and stationaize the signals. \n",
        "\n",
        "    Args:\n",
        "        input_path (str): directory to dataset.\n",
        "\n",
        "    Returns:\n",
        "        X (np.ndarray): features.\n",
        "        y (np.ndarray): ground truth.\n",
        "\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_path, nrows=251 if debug else None).groupby('provincename')\n",
        "\n",
        "    # X = df.iloc[:, 0:-1].values\n",
        "    province_data = []\n",
        "    for name, group in df:\n",
        "      X = group[[\"mean_max_temp_anomaly\" , \n",
        "              'mean_min_temp_anomaly', \n",
        "              'mean_temp_anomaly', \n",
        "              'extr_max_temp_anomaly', \n",
        "              'extr_min_temp_anomaly', \n",
        "              'total_precip_anomaly',\n",
        "              'total_rain_anomaly', \n",
        "              'total_snow_anomaly', \n",
        "              'snow_grnd_last_day_anomaly', \n",
        "              'co2_anomaly']].to_numpy()\n",
        "\n",
        "      #X = df.drop(['Date', 'provincename', 'growth_rate', 'GeoUID'], axis=1).to_numpy()\n",
        "      # y = df.iloc[:, -1].values\n",
        "      y = np.array(group['growth_rate'])\n",
        "\n",
        "      # print(X.shape)\n",
        "\n",
        "      # X[:,4] = np.where(X[:,4]<-10, 0, X[:,4])\n",
        "      ######################################################################################\n",
        "      # Stationarize each signal\n",
        "      # if diff:\n",
        "      #     y = y[1:]-y[0:-1]\n",
        "      #     X = X[1:,:] - X[0:-1,:]\n",
        "\n",
        "      ##########################################################################################\n",
        "      # Normalize every data\n",
        "      # y = (y - min(y))\n",
        "      # y /= max(y)\n",
        "\n",
        "      Z = np.array(group['GeoUID']).reshape(-1,1)\n",
        "      X = (X - np.min(X, axis=0))\n",
        "      X /= np.max(X, axis=0)\n",
        "      X = np.concatenate((Z, X), axis=1)\n",
        "      \n",
        "      province_data.append((X,y))\n",
        "\n",
        "    return province_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss1(nn.Module):\n",
        "    \"\"\"\n",
        "    A class which defines a custom loss function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, output, labels):\n",
        "        \"\"\"\n",
        "        The loss function implemented is e^(3*abs(y_pred - y_true))\n",
        "        \"\"\"\n",
        "        x = self.l1loss(output, labels)\n",
        "        loss = torch.mean(torch.expm1(torch.mul(x, 3)))\n",
        "        # print(\"X: \",x, \" Loss: \", loss)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CustomLoss2(nn.Module):\n",
        "    \"\"\"\n",
        "    A class which defines a custom loss function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, output, labels):\n",
        "        \"\"\"\n",
        "        The loss function implemented is e^(1.5*(y_pred - y_true)^2)\n",
        "        \"\"\"\n",
        "        x = self.l1loss(output, labels)\n",
        "        loss = torch.mean(torch.expm1(torch.mul(x ** 2, 1.5)))\n",
        "        # print(\"X: \",x, \" Loss: \", loss)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CustomLoss3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l2loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output, labels):\n",
        "        \"\"\"\n",
        "        The loss function implemented is 3*(y_pred - y_true)^2\n",
        "        \"\"\"\n",
        "        x = self.l2loss(output, labels)\n",
        "        loss = torch.mul(x, 3)\n",
        "        # print(\"X: \",x, \" Loss: \", loss)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "OKhcrEXy629m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder in DA_RNN.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, T,\n",
        "                 input_size,\n",
        "                 encoder_num_hidden,\n",
        "                 parallel=False):\n",
        "        \"\"\"\n",
        "        Initialize an encoder in DA_RNN.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        T: `int`\n",
        "            The number of timesteps to consider attention upon.\n",
        "        input_size:: `int`\n",
        "            The dimension of the input\n",
        "        encoder_num_hidden: `int`\n",
        "            The dimension of the encoder's hidden state\n",
        "        parallel: `bool`\n",
        "            If set to `True` the training will be done parallely.\n",
        "        \n",
        "        Returns\n",
        "        ------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_num_hidden = encoder_num_hidden\n",
        "        self.input_size = input_size\n",
        "        self.parallel = parallel\n",
        "        self.T = T\n",
        "\n",
        "        # Fig 1. Temporal Attention Mechanism: Encoder is LSTM\n",
        "        self.encoder_lstm = nn.LSTM(\n",
        "            input_size=self.input_size,\n",
        "            hidden_size=self.encoder_num_hidden,\n",
        "            num_layers = 1\n",
        "        )\n",
        "\n",
        "        # Construct Input Attention Mechanism via deterministic attention model\n",
        "        # Eq. 8: W_e[h_{t-1}; s_{t-1}] + U_e * x^k\n",
        "        self.encoder_attn = nn.Linear(\n",
        "            in_features=2 * self.encoder_num_hidden + self.T - 1,\n",
        "            out_features=1\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        The forward propagation for the encoder.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: `numpy.ndarray`\n",
        "            input data\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        X_tilde: `numpy.ndarray`\n",
        "            The input sequence of after forward pass thus after the application of attention\n",
        "        X_encoded: `numpy.ndarray`\n",
        "            The encoded sequence for the given input\n",
        "        \"\"\"\n",
        "\n",
        "        X_tilde = Variable(X.data.new(\n",
        "            X.size(0), self.T - 1, self.input_size).zero_())\n",
        "        X_encoded = Variable(X.data.new(\n",
        "            X.size(0), self.T - 1, self.encoder_num_hidden).zero_())\n",
        "\n",
        "        # Eq. 8, parameters not in nn.Linear but to be learnt\n",
        "        # v_e = torch.nn.Parameter(data=torch.empty(\n",
        "        #     self.input_size, self.T).uniform_(0, 1), requires_grad=True)\n",
        "        # U_e = torch.nn.Parameter(data=torch.empty(\n",
        "        #     self.T, self.T).uniform_(0, 1), requires_grad=True)\n",
        "\n",
        "        # h_n, s_n: initial states with dimention hidden_size\n",
        "        h_n = self._init_states(X)\n",
        "        s_n = self._init_states(X)\n",
        "\n",
        "        for t in range(self.T - 1):\n",
        "            # batch_size * input_size * (2 * hidden_size + T - 1)\n",
        "            x = torch.cat((h_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
        "                           s_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
        "                           X.permute(0, 2, 1)), dim=2)\n",
        "\n",
        "            x = self.encoder_attn(\n",
        "                x.view(-1, self.encoder_num_hidden * 2 + self.T - 1))\n",
        "\n",
        "            # get weights by softmax\n",
        "            alpha = F.softmax(x.view(-1, self.input_size))\n",
        "\n",
        "            # get new input for LSTM\n",
        "            x_tilde = torch.mul(alpha, X[:, t, :])\n",
        "\n",
        "            # Fix the warning about non-contiguous memory\n",
        "            # https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282\n",
        "            self.encoder_lstm.flatten_parameters()\n",
        "\n",
        "            # encoder LSTM\n",
        "            _, final_state = self.encoder_lstm(x_tilde.unsqueeze(0), (h_n, s_n))\n",
        "            h_n = final_state[0]\n",
        "            s_n = final_state[1]\n",
        "\n",
        "            X_tilde[:, t, :] = x_tilde\n",
        "            X_encoded[:, t, :] = h_n\n",
        "\n",
        "        return X_tilde, X_encoded\n",
        "\n",
        "\n",
        "    def _init_states(self, X):\n",
        "        \"\"\"\n",
        "        Initialize all 0 hidden states and cell states for encoder.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: `numpy.ndarray`\n",
        "            The input array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            initial_hidden_states\n",
        "        \"\"\"\n",
        "\n",
        "        # https://pytorch.org/docs/master/nn.html?#lstm\n",
        "        return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_())"
      ],
      "metadata": {
        "id": "TKYLpS6_65E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder in DA_RNN.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, T, decoder_num_hidden, encoder_num_hidden):\n",
        "        \"\"\"\n",
        "        Initialize an decoder in DA_RNN.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        T: `int`\n",
        "            The number of timesteps to consider attention upon.\n",
        "        decoder_num_hidden:: `int`\n",
        "            The dimension of the decoder's hidden state\n",
        "        encoder_num_hidden: `int`\n",
        "            The dimension of the encoder's hidden state\n",
        "        \n",
        "        Returns\n",
        "        ------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_num_hidden = decoder_num_hidden\n",
        "        self.encoder_num_hidden = encoder_num_hidden\n",
        "        self.T = T\n",
        "\n",
        "        self.attn_layer = nn.Sequential(\n",
        "            nn.Linear(2 * decoder_num_hidden + encoder_num_hidden, encoder_num_hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(encoder_num_hidden, 1)\n",
        "        )\n",
        "        self.lstm_layer = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=decoder_num_hidden\n",
        "        )\n",
        "        self.fc = nn.Linear(encoder_num_hidden + 1, 1)\n",
        "        self.fc_final = nn.Linear(decoder_num_hidden + encoder_num_hidden, 1)\n",
        "\n",
        "        self.fc.weight.data.normal_()\n",
        "\n",
        "\n",
        "    def forward(self, X_encoded, y_prev):\n",
        "        \"\"\"\n",
        "        The forward propagation for the decoder.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_encoded: `numpy.ndarray`\n",
        "            The input data after encoding.\n",
        "        y_prev: `numpy.ndarray`\n",
        "            The output in the previous timestep\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: `numpy.ndarray`\n",
        "            The predicted output for the current timestep\n",
        "        \"\"\"\n",
        "\n",
        "        d_n = self._init_states(X_encoded)\n",
        "        c_n = self._init_states(X_encoded)\n",
        "\n",
        "        for t in range(self.T - 1):\n",
        "\n",
        "            x = torch.cat((d_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n",
        "                           c_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n",
        "                           X_encoded), dim=2)\n",
        "\n",
        "            beta = F.softmax(self.attn_layer(\n",
        "                x.view(-1, 2 * self.decoder_num_hidden + self.encoder_num_hidden)).view(-1, self.T - 1))\n",
        "\n",
        "            # Eqn. 14: compute context vector\n",
        "            # batch_size * encoder_hidden_size\n",
        "            context = torch.bmm(beta.unsqueeze(1), X_encoded)[:, 0, :]\n",
        "            if t < self.T - 1:\n",
        "                # Eqn. 15\n",
        "                # batch_size * 1\n",
        "                y_tilde = self.fc(\n",
        "                    torch.cat((context, y_prev[:, t].unsqueeze(1)), dim=1))\n",
        "\n",
        "                # Eqn. 16: LSTM\n",
        "                self.lstm_layer.flatten_parameters()\n",
        "                _, final_states = self.lstm_layer(\n",
        "                    y_tilde.unsqueeze(0), (d_n, c_n))\n",
        "\n",
        "                d_n = final_states[0]  # 1 * batch_size * decoder_num_hidden\n",
        "                c_n = final_states[1]  # 1 * batch_size * decoder_num_hidden\n",
        "\n",
        "        # Eqn. 22: final output\n",
        "        y_pred = self.fc_final(torch.cat((d_n[0], context), dim=1))\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def _init_states(self, X):\n",
        "        \"\"\"\n",
        "        Initialize all 0 hidden states and cell states for encoder.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: `numpy.ndarray`\n",
        "            The input array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            initial_hidden_states\n",
        "        \"\"\"\n",
        "\n",
        "        # hidden state and cell state [num_layers*num_directions, batch_size, hidden_size]\n",
        "        # https://pytorch.org/docs/master/nn.html?#lstm\n",
        "        return Variable(X.data.new(1, X.size(0), self.decoder_num_hidden).zero_())"
      ],
      "metadata": {
        "id": "7LUUI8Vh68_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DA_rnn(nn.Module):\n",
        "    \"\"\"DARNN model.\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, T,\n",
        "                 encoder_num_hidden,\n",
        "                 decoder_num_hidden,\n",
        "                 batch_size,\n",
        "                 learning_rate,\n",
        "                 epochs,\n",
        "                 loss_func,\n",
        "                 train_size,\n",
        "                 parallel=False):\n",
        "        \"\"\"\n",
        "        DA_RNN model initialization.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X: `numpy.ndarray`\n",
        "            The input matrix containing all the timeseries data\n",
        "        y: `numpy.ndarray`\n",
        "            The predicted timeseries for the output\n",
        "        T: `int`\n",
        "            The number of timesteps to consider attention upon.\n",
        "        encoder_num_hidden: `int`\n",
        "            The dimension of the encoder's hidden state\n",
        "        decoder_num_hidden:: `int`\n",
        "            The dimension of the decoder's hidden state\n",
        "        batch_size: `int`\n",
        "            The batch size\n",
        "        learning_rate: `float`\n",
        "            The learning rate to be used\n",
        "        epochs: `int`\n",
        "            The number of epochs to run\n",
        "        loss_func: `nn.Module`\n",
        "            The loss function to be used\n",
        "        train_size: `float`\n",
        "            The percentage of samples to be used for training\n",
        "        parallel: `bool`\n",
        "            If set to `True` the training will be done parallely.\n",
        "        \n",
        "        Returns\n",
        "        ------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        super(DA_rnn, self).__init__()\n",
        "        self.encoder_num_hidden = encoder_num_hidden\n",
        "        self.decoder_num_hidden = decoder_num_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.parallel = parallel\n",
        "        self.shuffle = False\n",
        "        self.epochs = epochs\n",
        "        self.T = T\n",
        "        self.X = X[:,1:]\n",
        "        self.x_full = X\n",
        "        self.y_full = y\n",
        "        self.y = y\n",
        "        self.train_size = train_size\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        print(\"==> Use accelerator: \", self.device)\n",
        "\n",
        "        # Initialize the Encoder and Decoder\n",
        "        self.Encoder = Encoder(input_size=X.shape[1]-1,\n",
        "                               encoder_num_hidden=encoder_num_hidden,\n",
        "                               T=T).to(self.device)\n",
        "        self.Decoder = Decoder(encoder_num_hidden=encoder_num_hidden,\n",
        "                               decoder_num_hidden=decoder_num_hidden,\n",
        "                               T=T).to(self.device)\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = loss_func()\n",
        "\n",
        "        if self.parallel:\n",
        "            self.encoder = nn.DataParallel(self.encoder)\n",
        "            self.decoder = nn.DataParallel(self.decoder)\n",
        "\n",
        "        # Declare the opimizers\n",
        "        self.encoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad,\n",
        "                                                          self.Encoder.parameters()),\n",
        "                                            lr=self.learning_rate)\n",
        "        self.decoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad,\n",
        "                                                          self.Decoder.parameters()),\n",
        "                                            lr=self.learning_rate)\n",
        "\n",
        "        # Training set\n",
        "        #self.train_timesteps = int(self.X.shape[0] * self.train_size)\n",
        "        self.train_timesteps = int(251 * self.train_size)  \n",
        "        # self.y = self.y - np.mean(self.y[:self.train_timesteps])\n",
        "        self.input_size = X.shape[1]-1\n",
        "\n",
        "\n",
        "    def train(self, train_summary_writer):\n",
        "        \"\"\"\n",
        "        The training process.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_summary_writer\n",
        "            Tensorboard summary writer\n",
        "        \"\"\"\n",
        "\n",
        "        iter_per_epoch = int(np.ceil(self.train_timesteps * 1. / self.batch_size))\n",
        "        self.iter_losses = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.epoch_losses = np.zeros(self.epochs)\n",
        "\n",
        "        n_iter = 0\n",
        "        num_geouids = np.unique(self.x_full[:,0], axis = 0).shape[0]\n",
        "\n",
        "        # for i in range(num_geouids):\n",
        "        #   self.X = self.x_full[ i*251 : (i+1)*251 ,1:]\n",
        "        #   self.y = self.y_full[i*251: (i+1)*251]\n",
        "        #   for epoch in range(self.epochs):\n",
        "        for epoch in range(self.epochs):\n",
        "          for i in range(num_geouids):\n",
        "            self.X = self.x_full[ i*251 : (i+1)*251 ,1:]\n",
        "            self.y = self.y_full[i*251: (i+1)*251]\n",
        "            if self.shuffle:\n",
        "                ref_idx = np.random.permutation(self.train_timesteps - self.T)\n",
        "            else:\n",
        "                ref_idx = np.array(range(self.train_timesteps - self.T))\n",
        "\n",
        "            idx = 0\n",
        "\n",
        "            while (idx < self.train_timesteps):\n",
        "                # get the indices of X_train\n",
        "                indices = ref_idx[idx:(idx + self.batch_size)]\n",
        "                # x = np.zeros((self.T - 1, len(indices), self.input_size))\n",
        "                x = np.zeros((len(indices), self.T - 1, self.input_size))\n",
        "                y_prev = np.zeros((len(indices), self.T - 1))\n",
        "                y_gt = self.y[indices + self.T]\n",
        "\n",
        "                # format x into 3D tensor\n",
        "                for bs in range(len(indices)):\n",
        "                    x[bs, :, :] = self.X[indices[bs]:(indices[bs] + self.T - 1), :]\n",
        "                    y_prev[bs, :] = self.y[indices[bs]: (indices[bs] + self.T - 1)]\n",
        "\n",
        "                loss = self.train_forward(x, y_prev, y_gt)\n",
        "                self.iter_losses[int(epoch * iter_per_epoch + idx / self.batch_size)] = loss\n",
        "\n",
        "                idx += self.batch_size\n",
        "                n_iter += 1\n",
        "\n",
        "                if n_iter % 4000 == 0 and n_iter != 0:\n",
        "                    for param_group in self.encoder_optimizer.param_groups:\n",
        "                        param_group['lr'] = param_group['lr'] * 0.9\n",
        "                    for param_group in self.decoder_optimizer.param_groups:\n",
        "                        param_group['lr'] = param_group['lr'] * 0.9\n",
        "\n",
        "                self.epoch_losses[epoch] = np.mean(self.iter_losses[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "            \n",
        "            y_train_pred = self.test(on_train=True)\n",
        "            y_test_pred = self.test(on_train=False)[1:]\n",
        "\n",
        "            y_train_true = self.y[self.T : (len(y_train_pred) + self.T)]\n",
        "            y_test_true = self.y[self.T + len(y_train_pred) : (len(self.y) + 1)]\n",
        "\n",
        "            err_train = np.abs(y_train_true - y_train_pred)\n",
        "            mae_train = np.mean(err_train)\n",
        "            rmse_train = np.sqrt(np.mean(err_train ** 2))\n",
        "\n",
        "            err_test = np.abs(y_test_true - y_test_pred)\n",
        "            mae_test = np.mean(err_test)\n",
        "            rmse_test = np.sqrt(np.mean(err_test ** 2))\n",
        "            \n",
        "            y_pred = np.concatenate((y_train_pred, y_test_pred))\n",
        "\n",
        "            err_total = np.abs(self.y[self.T:]-y_pred)\n",
        "            mae_total = np.mean(err_total)\n",
        "            rmse_total = np.sqrt(np.mean(err_total ** 2))\n",
        "\n",
        "            with train_summary_writer.as_default():\n",
        "                # summary.scalar('Loss', loss, step=epoch)\n",
        "                summary.scalar('Epoch Average Loss', self.epoch_losses[epoch], step=epoch)\n",
        "                summary.scalar('MAE_total', mae_total, step=epoch)\n",
        "                summary.scalar('RMSE_total', rmse_total, step=epoch)\n",
        "                summary.scalar('MAE_train', mae_train, step=epoch)\n",
        "                summary.scalar('RMSE_train', rmse_train, step=epoch)\n",
        "                summary.scalar('MAE_test', mae_test, step=epoch)\n",
        "                summary.scalar('RMSE_test', rmse_test, step=epoch)\n",
        "                \n",
        "\n",
        "          print(\"Epochs: \", epoch, \" Iterations: \", n_iter,\n",
        "                  \" Epoch Loss: \", self.epoch_losses[epoch], \" Train MAE\", mae_train, \n",
        "                  \" Train RMSE\", rmse_train, \" Test MAE: \", mae_test, \" Test RMSE\", rmse_test)\n",
        "\n",
        "        y_train_pred = self.test(on_train=True)\n",
        "        y_test_pred = self.test(on_train=False)\n",
        "\n",
        "        y_pred = np.concatenate((y_train_pred, y_test_pred))\n",
        "        plt.ioff()\n",
        "        plt.figure()\n",
        "        plt.plot(range(1, 1 + len(self.y)), self.y, label=\"True\")\n",
        "        plt.plot(range(self.T, len(y_train_pred) + self.T),\n",
        "                    y_train_pred, label='Predicted - Train')\n",
        "        plt.plot(range(self.T + len(y_train_pred), len(self.y) + 1),\n",
        "                    y_test_pred, label='Predicted - Test')\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.savefig(\"test.png\")\n",
        "        img = plt.imread(\"test.png\")\n",
        "        os.remove(\"test.png\")\n",
        "\n",
        "            # # Save files in last iterations\n",
        "            # if epoch == self.epochs - 1:\n",
        "            #     np.savetxt('../loss.txt', np.array(self.epoch_losses), delimiter=',')\n",
        "            #     np.savetxt('../y_pred.txt',\n",
        "            #                np.array(self.y_pred), delimiter=',')\n",
        "            #     np.savetxt('../y_true.txt',\n",
        "            #                np.array(self.y_true), delimiter=',')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # input_attn = self.Encoder.encoder_attn.weight.cpu().numpy()\n",
        "            # temp_attn = self.Decoder.attn_layer[2].weight.cpu().numpy()\n",
        "            # print(input_attn.shape)\n",
        "            # print(temp_attn.shape)\n",
        "            with train_summary_writer.as_default():\n",
        "                # summary.histogram('Input Attention Weights', input_attn, step=1)\n",
        "                # summary.histogram('Temporal Attention Weights', temp_attn, step=1)\n",
        "                summary.image('Final Prediction Plot', np.expand_dims(img, 0), step=1)\n",
        "\n",
        "\n",
        "    def train_forward(self, X, y_prev, y_gt):\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder decoder network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: `numpy.ndarray`\n",
        "            The input array\n",
        "        y_prev: `numpy.ndarray`\n",
        "            The previous predicted value\n",
        "        y_gt: `numpy.ndarray`\n",
        "            Ground truth label\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "            The loss incurred at the current step\n",
        "        \"\"\"\n",
        "\n",
        "        # zero gradients\n",
        "        self.encoder_optimizer.zero_grad()\n",
        "        self.decoder_optimizer.zero_grad()\n",
        "\n",
        "        input_weighted, input_encoded = self.Encoder(\n",
        "            Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n",
        "        y_pred = self.Decoder(input_encoded, Variable(\n",
        "            torch.from_numpy(y_prev).type(torch.FloatTensor).to(self.device)))\n",
        "\n",
        "        y_true = Variable(torch.from_numpy(\n",
        "            y_gt).type(torch.FloatTensor).to(self.device))\n",
        "\n",
        "        y_true = y_true.view(-1, 1)\n",
        "        loss = self.criterion(y_pred, y_true)\n",
        "        loss.backward()\n",
        "\n",
        "        self.encoder_optimizer.step()\n",
        "        self.decoder_optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def test(self, on_train=False, inner_test=True, X_subset = None, y_subset = None):\n",
        "        \"\"\"\n",
        "        The test function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        on_train: `bool`\n",
        "            Whether to test on the training data or not\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "            The predicted value\n",
        "        \"\"\"\n",
        "\n",
        "        if on_train:\n",
        "            y_pred = np.zeros(self.train_timesteps - self.T + 1)\n",
        "        elif inner_test:\n",
        "            y_pred = np.zeros(self.X.shape[0] - self.train_timesteps)\n",
        "        else:\n",
        "          # redefine self.x and self.y. Is that it?\n",
        "          self.X = X_subset\n",
        "          self.y = y_subset\n",
        "          y_pred = np.zeros(self.X.shape[0] - self.train_timesteps)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(y_pred):\n",
        "            batch_idx = np.array(range(len(y_pred)))[i: (i + self.batch_size)]\n",
        "            X = np.zeros((len(batch_idx), self.T - 1, self.X.shape[1]))\n",
        "            y_history = np.zeros((len(batch_idx), self.T - 1))\n",
        "\n",
        "            for j in range(len(batch_idx)):\n",
        "                if on_train:\n",
        "                    X[j, :, :] = self.X[range(\n",
        "                        batch_idx[j], batch_idx[j] + self.T - 1), :]\n",
        "                    y_history[j, :] = self.y[range(\n",
        "                        batch_idx[j], batch_idx[j] + self.T - 1)]\n",
        "                else:\n",
        "                    X[j, :, :] = self.X[range(\n",
        "                        batch_idx[j] + self.train_timesteps - self.T, batch_idx[j] + self.train_timesteps - 1), :]\n",
        "                    y_history[j, :] = self.y[range(\n",
        "                        batch_idx[j] + self.train_timesteps - self.T, batch_idx[j] + self.train_timesteps - 1)]\n",
        "\n",
        "            y_history = Variable(torch.from_numpy(\n",
        "                y_history).type(torch.FloatTensor).to(self.device))\n",
        "            _, input_encoded = self.Encoder(\n",
        "                Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n",
        "            y_pred[i:(i + self.batch_size)] = self.Decoder(input_encoded,\n",
        "                                                           y_history).cpu().data.numpy()[:, 0]\n",
        "            i += self.batch_size\n",
        "\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "fj9yiP0q6_CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(debug=False):\n",
        "    \"\"\"\n",
        "    This is the main function which runs the entire pipeline of loading the data, defining the number of\n",
        "    training runs on the network, along with storing the model performance while training and testing for\n",
        "    easier access through tensorboard.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    debug: `bool`\n",
        "        If true the training is done in debug mode for a very small number of epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    dataroot = 'full_data.csv'\n",
        "\n",
        "    # Create the SummaryWriter object\n",
        "    train_summary_writer = SummaryWriter(log_dir='logs/train')\n",
        "    \n",
        "    # Define the list of hyperparameteres to run the training loops on and then the best possible\n",
        "    # set of hyperparameters can be chosen\n",
        "    parameters = dict(  batchsize = [32],\n",
        "                        nhidden_encoder = [32],\n",
        "                        nhidden_decoder = [32],\n",
        "                        ntimestep = [12],\n",
        "                        lr = [0.001],\n",
        "                        epochs = [5],\n",
        "                        # loss_func = [CustomLoss2, CustomLoss3],\n",
        "                        loss_func = [CustomLoss3],\n",
        "                        diff = [False]\n",
        "                    )\n",
        "\n",
        "    param_values = [v for v in parameters.values()]\n",
        "    print(\"Number of runs: \", len(list(product(*param_values))))\n",
        "\n",
        "    run_stats = []\n",
        "    runs = 0\n",
        "\n",
        "    for batchsize, nhidden_encoder, nhidden_decoder, ntimestep, lr, epochs, loss_func, diff in product(*param_values):\n",
        "#batchsize, nhidden_encoder, nhidden_decoder, ntimestep, lr, epochs, loss_func, diff = [32,32,32,12,0.001,1,CustomLoss1,False]\n",
        "\n",
        "\n",
        "        # Read dataset\n",
        "        print(\"==> Load dataset ...\")\n",
        "        province_data = read_data(dataroot, debug, diff)\n",
        "        # province_data = read_data(dataroot, True, diff)\n",
        "        train_size = 0.75\n",
        "        \n",
        "        runs += 1\n",
        "        if debug:\n",
        "            epochs = 2\n",
        "        \n",
        "        # Initialize model\n",
        "        print(\"==> Initialize DA-RNN model ...\")\n",
        "\n",
        "        #for prov in province_data:\n",
        "        for prov in province_data:\n",
        "          model = DA_rnn(\n",
        "              prov[0],\n",
        "              prov[1],\n",
        "              ntimestep,\n",
        "              nhidden_encoder,\n",
        "              nhidden_decoder,\n",
        "              batchsize,\n",
        "              lr,\n",
        "              epochs,\n",
        "              loss_func,\n",
        "              train_size,\n",
        "          )\n",
        "\n",
        "          # Name the run based on hyperparamter values\n",
        "          run_name = f'batch_size={batchsize} lr={lr} epochs = {epochs} nhidden_encoder = {nhidden_encoder} nhidden_decoder = {nhidden_decoder}'\n",
        "          run_name += f' ntimestep = {ntimestep} loss_func = {loss_func} diff = {diff}'\n",
        "\n",
        "          if debug:\n",
        "              run_name += ' Test'\n",
        "\n",
        "          # Store the logs\n",
        "          train_log_dir = 'logs/train/' + run_name\n",
        "          train_summary_writer = summary.create_file_writer(train_log_dir)\n",
        "\n",
        "          # Train\n",
        "          print(\"==> Start training ...\", runs)\n",
        "          print(run_name)\n",
        "          model.train(train_summary_writer)\n",
        "\n",
        "          print(\"==> Testing ...\")\n",
        "\n",
        "          num_geouids = np.unique(prov[0][:,0], axis = 0).shape[0]\n",
        "          err_list = []\n",
        "          mae_list = []\n",
        "          rmse_list = []\n",
        "\n",
        "          for i in range(num_geouids):\n",
        "            sub_X = prov[0][ i*251 : (i+1)*251 ,1:]\n",
        "            sub_y = prov[1][i*251: (i+1)*251]\n",
        "\n",
        "            # Prediction\n",
        "            y_pred = model.test(inner_test=False, X_subset = sub_X, y_subset = sub_y)\n",
        "\n",
        "            # Test statisitcs\n",
        "\n",
        "            # Modify to calculate errors for each GeoUID and then average over all of them\n",
        "            #y_true = prov[1][int(X.shape[0] * train_size):]\n",
        "            y_true = sub_y[int(sub_X.shape[0] * train_size):]\n",
        "\n",
        "            err = np.abs(y_true - y_pred)\n",
        "            mae = np.mean(err)\n",
        "            rmse = np.sqrt(np.mean(err ** 2))\n",
        "            \n",
        "            # Write training loss to TensorBoard\n",
        "            with train_summary_writer.as_default():\n",
        "              summary.scalar('Training Loss', np.mean(err), step=i)\n",
        "              summary.scalar('Training MAE', np.mean(mae), step=i)\n",
        "              summary.scalar('Training RMSE', np.mean(rmse), step=i)\n",
        "              \n",
        "            # Append Errors\n",
        "            err_list.append(err)\n",
        "            mae_list.append(mae)\n",
        "            rmse_list.append(rmse)\n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "          avg_err = sum(err_list)/len(err_list)\n",
        "          avg_mae = sum(mae_list)/len(mae_list)\n",
        "          avg_rmse = sum(rmse_list)/len(rmse_list)\n",
        "          #print(\"Average mean absolute error: \", mae, \"Average Root mean Square error: \", rmse)\n",
        "          print(\"Average mean absolute error: \", avg_mae, \"Average Root mean Square error: \", avg_rmse)\n",
        "          run_stats.append((avg_rmse, avg_mae, run_name))\n",
        "\n",
        "          # fig1 = plt.figure()\n",
        "          # plt.semilogy(range(len(model.iter_losses)), model.iter_losses)\n",
        "          # plt.savefig(\"1.png\")\n",
        "          # plt.close(fig1)\n",
        "\n",
        "          # fig2 = plt.figure()\n",
        "          # plt.semilogy(range(len(model.epoch_losses)), model.epoch_losses)\n",
        "          # plt.savefig(\"2.png\")\n",
        "          # plt.close(fig2)\n",
        "\n",
        "          # fig3 = plt.figure()\n",
        "          # plt.plot(y_pred, label='Predicted')\n",
        "          # plt.plot(model.y[model.train_timesteps:], label=\"True\")\n",
        "          # plt.legend(loc='upper left')\n",
        "          # plt.savefig(\"3.png\")\n",
        "          # plt.close(fig3)\n",
        "          # print('Finished Training')\n",
        "\n",
        "\n",
        "    # Display the best results\n",
        "    run_stats.sort()\n",
        "    print(\"Best runs: \")\n",
        "    for run in run_stats:\n",
        "        print(\"Name:\", run[2], \"RMSE \", run[0], \"MAE: \", run[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "O-j7UWXP7DqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "clw-PU5Ym4jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "n8fysIOjTuul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorboardX\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n"
      ],
      "metadata": {
        "id": "9YKaEITSmbb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}